# ğŸ“„ comments_extract.py - Documentation

## ğŸ¯ Purpose
Extracts comprehensive comment data (including replies) from Instagram posts using the post links generated by `post_link_extract.py`.

---

## ğŸ”§ How It Works

### Core Functionality
1. **Reuses Post Data**: Leverages existing `{username}_posts.csv` files
2. **Deep Comment Mining**: Extracts comments AND their replies
3. **Threaded Conversations**: Maintains parent-child comment relationships
4. **Multi-Account Support**: Distributes workload across scraper accounts

### Data Extraction Process
```
Post Links (CSV) â†’ Individual Posts â†’ Comments â†’ Replies â†’ Structured CSV
```

---

## ğŸ“Š Output Format

Creates `{username}_posts_comments.csv` with detailed comment data:

| Column | Description | Example |
|--------|-------------|---------|
| Username | Target account | "travel_blogger" |
| Followers Count | Account followers | 15000 |
| Total Posts | Total post count | 450 |
| Post ID | Instagram media ID | "12345_67890" |
| Post Link | Instagram URL | "instagram.com/p/ABC123/" |
| Number of Post | Sequential post number | 1, 2, 3... |
| Comment Count | Post's total comments | 89 |
| Like Count | Post's total likes | 1250 |
| Date of Post | When post was created | "2024-01-15 14:30:00" |
| Commentor Username | Who made the comment | "john_doe" |
| Comment Text | The actual comment | "Amazing photo! ğŸ˜" |
| Comment Date | When comment was made | "2024-01-15 16:45:30" |
| Comment Likes Count | Likes on the comment | 25 |
| Parent Comment ID | For replies (empty for top-level) | "comment_123" |

---

## ğŸ—ï¸ Comment Structure

### Hierarchical Organization
```
ğŸ“ Top-level Comment 1
â”œâ”€â”€ ğŸ’¬ Reply 1.1
â”œâ”€â”€ ğŸ’¬ Reply 1.2
â””â”€â”€ ğŸ’¬ Reply 1.3

ğŸ“ Top-level Comment 2  
â”œâ”€â”€ ğŸ’¬ Reply 2.1
â””â”€â”€ ğŸ’¬ Reply 2.2
    â””â”€â”€ ğŸ’¬ Sub-reply 2.2.1
```

### Parent-Child Tracking
- **Top-level comments**: `Parent Comment ID` = empty
- **Replies**: `Parent Comment ID` = original comment's ID
- **Sub-replies**: Linked to immediate parent comment

---

## âš™ï¸ Configuration

### Dependencies
The script requires the same setup as `post_link_extract.py`:

#### Required Files
- `accounts.csv` - Target usernames
- `.env` - Scraper account credentials  
- `{username}_posts.csv` - Generated by post extraction

#### Configuration Variables
```python
POST_DELAY_RANGE = (1, 3)      # Delay between posts
BIG_PAUSE_RANGE = (20, 40)     # Longer pauses for rate limiting
MAX_ATTEMPTS_PER_ACCOUNT = 2   # Retry attempts before switching
```

---

## ğŸš€ Usage Instructions

### Prerequisites
âœ… **Must run `post_link_extract.py` first**
âœ… **Verify `{username}_posts.csv` files exist**  
âœ… **Ensure scraper accounts are active**

### Execution

#### Windows
```cmd
python comments_extract.py
```

#### macOS/Linux
```bash
python3 comments_extract.py
```

### Interactive Flow
1. **Enter CSV Path**: Provide path to accounts.csv
2. **Progress Monitoring**: Watch real-time extraction progress
3. **File Generation**: CSV files created automatically

---

## ğŸ“ˆ Progress Tracking

### Visual Progress Indicators
```
â³ username: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€â”€â”€â”€] 85% | ğŸ“¦ 85/100
```

### Status Messages
- ğŸ”‘ **Using scraper account**: Shows active account
- âœ… **Login successful**: Authentication confirmed  
- ğŸ“‚ **Found posts.csv**: Reusing existing post data
- âš ï¸ **Posts.csv not found**: Falling back to direct scraping
- âŒ **Error on post**: Individual post processing issue

---

## ğŸ”„ Smart Data Reuse

### Efficiency Features
```python
# Checks for existing post data
posts_file = f"{username}_posts.csv"
if os.path.exists(posts_file):
    print(f"ğŸ“‚ Found {posts_file}, reusing post links...")
```

### Fallback Mechanism
If post CSV doesn't exist:
1. **Direct Scraping**: Extracts posts on-the-fly
2. **Same Output**: Maintains consistent data format
3. **Performance**: Slower but complete extraction

---

## âš ï¸ Error Handling & Recovery

### Common Issues

**Missing Post Files**
```
âš ï¸ username_posts.csv not found, scraping posts directly...
```
- **Impact**: Slower processing
- **Solution**: Run post extraction first

**Login Failures**
```
âš ï¸ Login failed: Challenge required
```  
- **Action**: Switches to next scraper account
- **Recovery**: Automatic account rotation

**Rate Limiting**
```
âŒ Error on post: Too many requests
```
- **Response**: Increased delays between requests
- **Prevention**: Built-in pause mechanisms

### Recovery Mechanisms
- **Checkpoint System**: Saves progress continuously
- **Account Rotation**: Switches blocked accounts automatically
- **Graceful Degradation**: Continues with available accounts

---

## ğŸ“Š Data Quality & Completeness

### What Gets Extracted
âœ… **All comment types**: Text, emoji, mentions  
âœ… **Complete threads**: Comments + all reply levels  
âœ… **Engagement metrics**: Like counts for comments  
âœ… **Timestamps**: Precise creation times  
âœ… **User data**: Commenter usernames and relationships

### Potential Limitations
âš ï¸ **Private accounts**: Comments not accessible  
âš ï¸ **Deleted content**: Historical comments may be missing  
âš ï¸ **Rate limits**: May miss recent comments on high-activity posts  
âš ï¸ **Account restrictions**: Some comments might be hidden

---

## ğŸ•’ Processing Time Expectations

### Typical Durations
| Account Size | Posts | Expected Time |
|-------------|-------|---------------|
| Small (< 100 posts) | 50-100 | 30-45 minutes |
| Medium (100-500) | 200-500 | 1-2 hours |
| Large (500+) | 500+ | 2-4 hours |

### Factors Affecting Speed
- **Comment density**: Posts with many comments take longer
- **Reply depth**: Deep conversation threads increase time
- **Account status**: Rate limiting varies by scraper account
- **Network speed**: Connection quality impacts performance

---

## ğŸ” Advanced Features

### Parallel Processing
```python
# Distributes users across multiple accounts
with Pool(processes=len(chunked_args)) as pool:
    results = pool.map(process_chunk, chunked_args)
```

### Memory Management
- **Streaming writes**: Data written immediately to avoid memory issues
- **Chunk processing**: Large accounts processed in segments
- **Progress persistence**: Checkpoints prevent data loss

### Error Tolerance
```python
# Continues processing despite individual post errors
except Exception as e:
    print(f"\nâŒ Error on post {url}: {str(e)}")
    status = "error"
    continue  # Moves to next post
```

---

## ğŸ’¡ Best Practices

### Before Running
1. **Verify post extraction**: Ensure CSV files exist
2. **Check account status**: Confirm scraper accounts work
3. **Plan timing**: Comments take significantly longer than posts
4. **Monitor storage**: Large datasets require substantial disk space

### During Execution
1. **Don't interrupt**: Let the process complete naturally
2. **Monitor logs**: Watch for error patterns
3. **Account rotation**: Be prepared to add fresh accounts
4. **Patience**: Comment extraction is inherently slow

### After Completion
1. **Verify outputs**: Check generated CSV files
2. **Data validation**: Ensure comment counts match expectations
3. **Backup files**: Save important data before organization
4. **Proceed to organize**: Use organize.py to structure data

---

## ğŸ”— Integration Points

### Input Dependencies
- **post_link_extract.py** â†’ Provides post URLs and metadata
- **accounts.csv** â†’ Target usernames list
- **.env** â†’ Scraper account credentials

### Output Usage  
- **organize.py** â†’ Structures comment files into folders
- **Data analysis** â†’ CSV format ready for pandas/Excel
- **Further processing** â†’ Can be input for sentiment analysis
---
Author: Saad Makki\
Email: saadmakki116@gmail.com
---

**[â¬…ï¸ Back to Main README](../README.md)**