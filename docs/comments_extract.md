# 📄 comments_extract.py - Documentation

## 🎯 Purpose
Extracts comprehensive comment data (including replies) from Instagram posts using the post links generated by `post_link_extract.py`.

---

## 🔧 How It Works

### Core Functionality
1. **Reuses Post Data**: Leverages existing `{username}_posts.csv` files
2. **Deep Comment Mining**: Extracts comments AND their replies
3. **Threaded Conversations**: Maintains parent-child comment relationships
4. **Multi-Account Support**: Distributes workload across scraper accounts

### Data Extraction Process
```
Post Links (CSV) → Individual Posts → Comments → Replies → Structured CSV
```

---

## 📊 Output Format

Creates `{username}_posts_comments.csv` with detailed comment data:

| Column | Description | Example |
|--------|-------------|---------|
| Username | Target account | "travel_blogger" |
| Followers Count | Account followers | 15000 |
| Total Posts | Total post count | 450 |
| Post ID | Instagram media ID | "12345_67890" |
| Post Link | Instagram URL | "instagram.com/p/ABC123/" |
| Number of Post | Sequential post number | 1, 2, 3... |
| Comment Count | Post's total comments | 89 |
| Like Count | Post's total likes | 1250 |
| Date of Post | When post was created | "2024-01-15 14:30:00" |
| Commentor Username | Who made the comment | "john_doe" |
| Comment Text | The actual comment | "Amazing photo! 😍" |
| Comment Date | When comment was made | "2024-01-15 16:45:30" |
| Comment Likes Count | Likes on the comment | 25 |
| Parent Comment ID | For replies (empty for top-level) | "comment_123" |

---

## 🏗️ Comment Structure

### Hierarchical Organization
```
📝 Top-level Comment 1
├── 💬 Reply 1.1
├── 💬 Reply 1.2
└── 💬 Reply 1.3

📝 Top-level Comment 2  
├── 💬 Reply 2.1
└── 💬 Reply 2.2
    └── 💬 Sub-reply 2.2.1
```

### Parent-Child Tracking
- **Top-level comments**: `Parent Comment ID` = empty
- **Replies**: `Parent Comment ID` = original comment's ID
- **Sub-replies**: Linked to immediate parent comment

---

## ⚙️ Configuration

### Dependencies
The script requires the same setup as `post_link_extract.py`:

#### Required Files
- `accounts.csv` - Target usernames
- `.env` - Scraper account credentials  
- `{username}_posts.csv` - Generated by post extraction

#### Configuration Variables
```python
POST_DELAY_RANGE = (1, 3)      # Delay between posts
BIG_PAUSE_RANGE = (20, 40)     # Longer pauses for rate limiting
MAX_ATTEMPTS_PER_ACCOUNT = 2   # Retry attempts before switching
```

---

## 🚀 Usage Instructions

### Prerequisites
✅ **Must run `post_link_extract.py` first**
✅ **Verify `{username}_posts.csv` files exist**  
✅ **Ensure scraper accounts are active**

### Execution

#### Windows
```cmd
python comments_extract.py
```

#### macOS/Linux
```bash
python3 comments_extract.py
```

### Interactive Flow
1. **Enter CSV Path**: Provide path to accounts.csv
2. **Progress Monitoring**: Watch real-time extraction progress
3. **File Generation**: CSV files created automatically

---

## 📈 Progress Tracking

### Visual Progress Indicators
```
⏳ username: [█████████████████─────] 85% | 📦 85/100
```

### Status Messages
- 🔑 **Using scraper account**: Shows active account
- ✅ **Login successful**: Authentication confirmed  
- 📂 **Found posts.csv**: Reusing existing post data
- ⚠️ **Posts.csv not found**: Falling back to direct scraping
- ❌ **Error on post**: Individual post processing issue

---

## 🔄 Smart Data Reuse

### Efficiency Features
```python
# Checks for existing post data
posts_file = f"{username}_posts.csv"
if os.path.exists(posts_file):
    print(f"📂 Found {posts_file}, reusing post links...")
```

### Fallback Mechanism
If post CSV doesn't exist:
1. **Direct Scraping**: Extracts posts on-the-fly
2. **Same Output**: Maintains consistent data format
3. **Performance**: Slower but complete extraction

---

## ⚠️ Error Handling & Recovery

### Common Issues

**Missing Post Files**
```
⚠️ username_posts.csv not found, scraping posts directly...
```
- **Impact**: Slower processing
- **Solution**: Run post extraction first

**Login Failures**
```
⚠️ Login failed: Challenge required
```  
- **Action**: Switches to next scraper account
- **Recovery**: Automatic account rotation

**Rate Limiting**
```
❌ Error on post: Too many requests
```
- **Response**: Increased delays between requests
- **Prevention**: Built-in pause mechanisms

### Recovery Mechanisms
- **Checkpoint System**: Saves progress continuously
- **Account Rotation**: Switches blocked accounts automatically
- **Graceful Degradation**: Continues with available accounts

---

## 📊 Data Quality & Completeness

### What Gets Extracted
✅ **All comment types**: Text, emoji, mentions  
✅ **Complete threads**: Comments + all reply levels  
✅ **Engagement metrics**: Like counts for comments  
✅ **Timestamps**: Precise creation times  
✅ **User data**: Commenter usernames and relationships

### Potential Limitations
⚠️ **Private accounts**: Comments not accessible  
⚠️ **Deleted content**: Historical comments may be missing  
⚠️ **Rate limits**: May miss recent comments on high-activity posts  
⚠️ **Account restrictions**: Some comments might be hidden

---

## 🕒 Processing Time Expectations

### Typical Durations
| Account Size | Posts | Expected Time |
|-------------|-------|---------------|
| Small (< 100 posts) | 50-100 | 30-45 minutes |
| Medium (100-500) | 200-500 | 1-2 hours |
| Large (500+) | 500+ | 2-4 hours |

### Factors Affecting Speed
- **Comment density**: Posts with many comments take longer
- **Reply depth**: Deep conversation threads increase time
- **Account status**: Rate limiting varies by scraper account
- **Network speed**: Connection quality impacts performance

---

## 🔍 Advanced Features

### Parallel Processing
```python
# Distributes users across multiple accounts
with Pool(processes=len(chunked_args)) as pool:
    results = pool.map(process_chunk, chunked_args)
```

### Memory Management
- **Streaming writes**: Data written immediately to avoid memory issues
- **Chunk processing**: Large accounts processed in segments
- **Progress persistence**: Checkpoints prevent data loss

### Error Tolerance
```python
# Continues processing despite individual post errors
except Exception as e:
    print(f"\n❌ Error on post {url}: {str(e)}")
    status = "error"
    continue  # Moves to next post
```

---

## 💡 Best Practices

### Before Running
1. **Verify post extraction**: Ensure CSV files exist
2. **Check account status**: Confirm scraper accounts work
3. **Plan timing**: Comments take significantly longer than posts
4. **Monitor storage**: Large datasets require substantial disk space

### During Execution
1. **Don't interrupt**: Let the process complete naturally
2. **Monitor logs**: Watch for error patterns
3. **Account rotation**: Be prepared to add fresh accounts
4. **Patience**: Comment extraction is inherently slow

### After Completion
1. **Verify outputs**: Check generated CSV files
2. **Data validation**: Ensure comment counts match expectations
3. **Backup files**: Save important data before organization
4. **Proceed to organize**: Use organize.py to structure data

---

## 🔗 Integration Points

### Input Dependencies
- **post_link_extract.py** → Provides post URLs and metadata
- **accounts.csv** → Target usernames list
- **.env** → Scraper account credentials

### Output Usage  
- **organize.py** → Structures comment files into folders
- **Data analysis** → CSV format ready for pandas/Excel
- **Further processing** → Can be input for sentiment analysis
---
Author: Saad Makki\
Email: saadmakki116@gmail.com
---

**[⬅️ Back to Main README](../README.md)**